{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "class WCELossFunc(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha, beta, num_class):\n",
    "        super(WCELossFunc, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, scores, target):\n",
    "        eps = 0\n",
    "        pos_count = torch.sum(target).detach()\n",
    "        total = target.size(0) * target.size(1) + 1\n",
    "        weight_pos = total / (pos_count + 1)\n",
    "        weight_neg = total / (total - pos_count)\n",
    "\n",
    "        loss_list = torch.zeros(len(scores), self.num_class).to(scores.device)\n",
    "\n",
    "        probs = torch.sigmoid(scores)\n",
    "        for i in range(len(scores)):\n",
    "            for j in range(self.num_class):\n",
    "                loss_list[i][j] = -weight_pos * target[i][j] * torch.pow((1 - probs[i][j]), self.beta) * torch.log(probs[i][j] + eps)\\\n",
    "                                  - weight_neg * (1 - target[i][j]) * torch.pow(probs[i][j], self.beta) * torch.log(1 - probs[i][j] + eps)\n",
    "        loss = torch.mean(loss_list)\n",
    "        return loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class WCELossFuncMy(nn.Module):\n",
    "\n",
    "    def __init__(self, alpha, beta, num_class):\n",
    "        super(WCELossFuncMy, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.num_class = num_class\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        pos_count = torch.sum(target) + 1\n",
    "        total = target.size(0) * target.size(1) + 1\n",
    "        weight_pos = total / pos_count  # weight is neg/pos\n",
    "        weight_neg = total / (total - pos_count + 1)  # weight is neg/pos\n",
    "        output = torch.sigmoid(output)\n",
    "        output = output.clamp(min=1e-5, max=1-1e-5)\n",
    "\n",
    "        loss = -weight_pos * (target * torch.log(output)) * torch.pow((1 - output), self.beta) - \\\n",
    "               torch.pow(output, self.beta) * weight_neg * ((1 - target) * torch.log(1 - output))\n",
    "        return torch.mean(loss)\n",
    "        # pos_weight = torch.ones(target.size(1), device=target.device) * weight_factor\n",
    "        # loss = F.binary_cross_entropy_with_logits(scores, target, pos_weight=pos_weight, reduction=\"none\")\n",
    "        # pt = torch.exp(-loss)\n",
    "        # F_loss = self.alpha * (1-pt)**self.beta * loss\n",
    "        # return torch.mean(F_loss)\n",
    "        # return F.binary_cross_entropy_with_logits(scores, target, pos_weight=pos_weight)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.cuda.FloatTensor{[64, 64, 32]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (3)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [12]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m y_labels \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandint(high\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, size\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m16\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m64\u001B[39m,\u001B[38;5;241m64\u001B[39m,\u001B[38;5;241m32\u001B[39m), dtype\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mfloat)\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[0;32m     10\u001B[0m logits \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mrandn((\u001B[38;5;241m16\u001B[39m,\u001B[38;5;241m2\u001B[39m,\u001B[38;5;241m64\u001B[39m,\u001B[38;5;241m64\u001B[39m,\u001B[38;5;241m32\u001B[39m))\u001B[38;5;241m.\u001B[39mcuda()\n\u001B[1;32m---> 11\u001B[0m l1 \u001B[38;5;241m=\u001B[39m \u001B[43mloss1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_labels\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     12\u001B[0m l2 \u001B[38;5;241m=\u001B[39m loss2(logits, y_labels)\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(l1)\n",
      "File \u001B[1;32mD:\\anaconda\\envs\\3DUnet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1098\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1099\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1100\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1101\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1102\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28minput\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1103\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1104\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [7]\u001B[0m, in \u001B[0;36mWCELossFunc.forward\u001B[1;34m(self, scores, target)\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(scores)):\n\u001B[0;32m     20\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m j \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_class):\n\u001B[1;32m---> 21\u001B[0m         loss_list[i][j] \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m-\u001B[39mweight_pos \u001B[38;5;241m*\u001B[39m target[i][j] \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mpow((\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m probs[i][j]), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta) \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mlog(probs[i][j] \u001B[38;5;241m+\u001B[39m eps)\\\n\u001B[0;32m     22\u001B[0m                           \u001B[38;5;241m-\u001B[39m weight_neg \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m target[i][j]) \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mpow(probs[i][j], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbeta) \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mlog(\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m probs[i][j] \u001B[38;5;241m+\u001B[39m eps)\n\u001B[0;32m     23\u001B[0m loss \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmean(loss_list)\n\u001B[0;32m     24\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m loss\n",
      "\u001B[1;31mRuntimeError\u001B[0m: expand(torch.cuda.FloatTensor{[64, 64, 32]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"_pydevd_bundle\\pydevd_cython_win32_39_64.pyx\", line 1034, in _pydevd_bundle.pydevd_cython_win32_39_64.PyDBFrame.trace_dispatch\n",
      "  File \"D:\\pycharm\\PyCharm 2022.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\", line 144, in cmd_step_over\n",
      "    if _is_inside_jupyter_cell(frame, pydb):\n",
      "  File \"D:\\pycharm\\PyCharm 2022.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\", line 209, in _is_inside_jupyter_cell\n",
      "    if is_cell_filename(filename):\n",
      "  File \"D:\\pycharm\\PyCharm 2022.1\\plugins\\python\\helpers-pro\\jupyter_debug\\pydev_jupyter_plugin.py\", line 220, in is_cell_filename\n",
      "    ipython_shell = get_ipython()\n",
      "NameError: name 'get_ipython' is not defined\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    alpha = 0.25\n",
    "    beta = 2\n",
    "    num_class = 14\n",
    "    weight_neg = 20\n",
    "    weight_pos = 25\n",
    "    loss1 = WCELossFunc(alpha=alpha, beta=beta, num_class=num_class)\n",
    "    loss2 = WCELossFuncMy(alpha=alpha, beta=beta, num_class=num_class)\n",
    "    y_labels = torch.randint(high=2, size=(16,2,64,64,32), dtype=torch.float).cuda()\n",
    "    logits = torch.randn((16,2,64,64,32)).cuda()\n",
    "    l1 = loss1(logits, y_labels)\n",
    "    l2 = loss2(logits, y_labels)\n",
    "    print(l1)\n",
    "    print(l2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}